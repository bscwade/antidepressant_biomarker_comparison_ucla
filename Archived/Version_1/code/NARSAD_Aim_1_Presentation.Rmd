---
title: "NARSAD_Aim_1_Presentation"
author: "Benjamin Wade"
date: "5/10/2020"
output: html_document
---


# Outline

* Project Aims
* Outcomes
* Sample Outline
* Predictors
* Modeling Approach
* Coarse Grid Search for Hyperparameters
    + Classifier Selection
    + Feature Correlatoin Cutoff

* Refined Grid Search: Manual Appraoch
* Refined Grid Search: Adaptive Approach
* Comparison of Approaches
* Outline of Top-performing Models
    + ECT models within treatment arm
        - RRS: Reflection
        - HDRS-6
    + Ketamine models within treatment arm
        - RRS: Reflection
        - HDRS-6
    + Sleep Deprivation models within treatment arm
        - RRS: Reflection
        - HDRS-6

* Model Performances Across Treatment Arms
* Error Analysis of Top-performing Models
* Next Steps

# Project Aims
**Aim 1**: Predict degree of improvement in ruminative and depressive symptoms: RRS & HDRS-6.

**Aim 2**: Determine which treatment-specific models predict across treatment arms.


```{r setup, include=FALSE}
require(ggplot2)
require(caret)
require(reshape2)
require(randomForest)
require(xgboost)
require(pdp)
require(png)
require(grid)
require(gridExtra)
require(factoextra)
require(stringr)
require(plyr)
require(knitr)
require(pander)
```

## Outcomes

* Rumination Reflection Scale - Reflection dimension (RRSR)

* Hamilton Depression Rating Scale (HDRS) 6-item Version
    + Depressed Mood
    + Guilt
    + Work & Interests
    + Psychomotor Retardation
    + Psychic Anxiety
    + General Somatic Symptoms

# Sample outline
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
load('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/rrsr_end_of_treatment.Rdata')
dem_rrsr <- df; rm(df)

load('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/hdrs6_end_of_treatment.Rdata')
dem_hdrs6 <- df; rm(df)

dem <- data.frame(group=dem_rrsr$group, 
                  age=dem_rrsr$cont_age,
                  sex=dem_rrsr$cont_sex,
                  rrsr_baseline=dem_rrsr$outcome_baseline,
                  rrsr_delta=dem_rrsr$outcome,
                  hdrs6_baseline=dem_hdrs6$outcome_baseline,
                  hdrs6_delta=dem_hdrs6$outcome)
rm(dem_rrsr)
rm(dem_hdrs6)

dem$rrsr_percent_change <- (dem$rrsr_delta/dem$rrsr_baseline)
dem$hdrs_percent_change <- (dem$hdrs6_delta/dem$hdrs6_baseline)
mplt <- melt(dem, id.vars=c('group', 'age', 'sex', 'rrsr_baseline', 'rrsr_delta', 'hdrs6_baseline', 'hdrs6_delta'))

ggplot(mplt, aes(group, value, fill = group)) + 
  geom_boxplot() + 
  theme_bw() + 
  facet_grid(~variable) + 
  ggtitle("Symptom Percent Changes by Treatment Arm") + 
  geom_hline(yintercept = 0, col='red') + 
  ylab("Symptom Percent Change") + 
  xlab("Treatment Group")

knitr::kable(ddply(dem, .(group), summarise, n=length(sex), mean_age=mean(age), sd_age=sd(age), percent_male=(sum(sex)/length(sex)),
      hdrs6_baseline=mean(hdrs6_baseline), hdrs6_percent_change=(mean(hdrs6_delta)/mean(hdrs6_baseline)),
      rrsr_baseline=mean(rrsr_baseline), rrsr_percent_change=(mean(rrsr_delta)/mean(rrsr_baseline))), 
      caption = 'Demographics Table')


dem$hdrs6_percent_delta <- (dem$hdrs6_delta/dem$hdrs6_baseline)
dem$rrsr_percent_delta <- (dem$rrsr_delta/dem$rrsr_baseline)
anova_hdrs <- aov(dem$hdrs6_percent_delta~dem$group)
anova_rrsr <- aov(dem$rrsr_percent_delta~dem$group)

hdrs_ph <- TukeyHSD(anova_hdrs)
rrsr_ph <- TukeyHSD(anova_rrsr)
```

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
kable(hdrs_ph$`dem$group`, caption = 'Treatment-by-HDRS % Change ANOVA Table')
```

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
kable(rrsr_ph$`dem$group`, caption = 'Treatment-by-RRSR % Change ANOVA Table')
```

## Predictors

* Demographics
    + Age
    + Sex
    + Duration of lifetime illness in years
    + Number of previous episodes
  
* Baseline Symptom Severity
    + Models with and without to determine reliance on baseline values
  
* Multimodal Imaging
    + Regional Cortical Thickness
    + Subcortical Volume
    + Diffusion (FA, RD, AD, MD, Kurtosis)
    + Between-network ICA-based connectivity
    + Global connectivity of ICA nodes

![Global Connectivity](/ifshome/bwade/NARSAD/Aim_1/figures/NDD_Illustration_2.png)
  
# Modeling approach

1. Coarse grid search over hyperparameters
    + **Pick classifier**: Random Forest vs. SVM vs. Gradient Boosted Trees (simple coarse grid search for each with 10-fold cross validation)
    + **Degree of feature filtering**: We have ~16K features. I removed highly correlated features outright with |r| cutoffs {.1, .2, .3, .4, .5}

2. Given results of coarse search, tune model-specific parameters and process hyperparameters more finely
    + Manual approach with more hands-on feature selection
        - Removal of global connectivity features with zero mode and little variation
        - Removal of features with near-zero variance (similar to above)
        - Iterative tuning of model-specific parameters: Bigger knobs first. 
        
    + Adaptive grid search with GLS (Guided Local Search?) algorithm
        - Removal of global connectivity features with zero mode and little variation
        - Removal of features with near-zero variance (similar to above)
        - Joint removal of highly correlated features

### Machine learning basics

* Supervised machine learning is the process of training a model on labeled data and using it to make predictions in data not used for training
* Hundreds of models available that vary by complexity and purpose (classification, regression, clustering)
* Models have specific sets of parameters like beta weights in linear regreession. These require tuning to find optimal settings/combinations. 
* The process of finding optimal parameter combinations is usually done with some form of grid search
![Grid search example](/ifshome/bwade/NARSAD/Aim_1/figures/grid_search_example.png)
* Bias-variance tradeoff:
    + Overly simple models usually don't capture complex patterns in the data and are said to be 'underfit' and have 'high bias'.
    + Overly complex models capture noise/idiosyncrasies of the training data and are said to be 'overfit' or have 'high variance'.

![Under and overfitting](/ifshome/bwade/NARSAD/Aim_1/figures/over_under_fitting.png)

* Finding the right amount of complexity is usually done with cross validation
    + Train the model on a majority of the dataset 
    + Test fitted model on held-out data
    + Usually this is repeated many times and results are averaged

![Cross validation](/ifshome/bwade/NARSAD/Aim_1/figures/cross_validation_example.png)


# Coarse grid search: Classifier comparison

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
grid <- expand.grid(classifier=c('rf', 'xgbTree', 'svmRadial'),
                    cutoff=c(0.1, 0.2, 0.3, 0.4, 0.5),
                    ndthresh=c(0.5, 0.6, 0.7, 0.8, 0.9),
                    baseline=c(TRUE, FALSE),
                    y=c('rrsr', 'hdrs6'),
                    diff_drop=FALSE,
                    rmterms=TRUE)


perf_coarse <- lapply(1:nrow(grid), function(r){
  
  flist <- dir('/ifshome/bwade/NARSAD/Aim_1/results/gridsearch/', full.names = TRUE)
  
  f <- grep(paste0('ndthresh-', grid[r, 'ndthresh']), grep(grid[r, 'y'], grep(paste0('baseline-', grid[r, 'baseline']), grep(paste0('cutoff-', grid[r, 'cutoff']), grep(grid[r, 'classifier'], flist, value = T), value = T), value = T), value = T), value = T)
  
  if(length(f)==0){
    return(NA)
  }else{
    
    
    load(f)
    
    cm <- list()
    
    for(a in 1:3){
      
      pred <- fits[[a]]$mod$pred
      
      cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
      
    }
    
    return(cm)
  }
  
})

grid$performance_ect <- sapply(perf_coarse, function(x) tryCatch(x[[1]], error=function(e) NA))
grid$performance_ski <- sapply(perf_coarse, function(x) tryCatch(x[[2]], error=function(e) NA))
grid$performance_tsd <- sapply(perf_coarse, function(x) tryCatch(x[[3]], error=function(e) NA))

m <- melt(grid, id.vars=c('classifier', 'cutoff', 'ndthresh', 'baseline','y','diff_drop', 'rmterms'))

m$cutoff <- factor(m$cutoff, levels=c(0.1, 0.2, 0.3, 0.4, 0.5))
m$cutoff <- as.factor(m$cutoff)
m$ndthresh <- as.factor(m$ndthresh)

ggplot(m, aes(classifier, value, fill=classifier)) + 
  geom_boxplot() + facet_grid(baseline~y) + theme_bw() + 
  ylab('Predicted vs. Actual Change Correlation') + xlab('Classifier') + 
  ggtitle('Hyperparameter Tuning: Classifier Selection') + 
  theme_bw()

ggplot(m, aes(cutoff, value, fill=cutoff)) + 
  geom_boxplot() + facet_grid(baseline~y) + theme_bw() + 
  ylab('Predicted vs. Actual Change Correlation') + xlab('Correlation Cutoff') + 
  ggtitle('Hyperparameter Tuning: Correlation Threshold') + 
  theme_bw()

ggplot(m, aes(ndthresh, value, fill=ndthresh)) + 
  geom_boxplot() + facet_grid(baseline~y) + theme_bw() + 
  ylab('Predicted vs. Actual Change Correlation') + xlab('Node Degree Variance Threshold') + 
  ggtitle('Hyperparameter Tuning: Node Degree Exclusion') + 
  theme_bw()
```
**Result**: Gradient boosted trees consistently outperformed RFs and SVMs. Lower |r| thresholds tend to perform better. 

![Gradient Boosted Trees Illustration](/ifshome/bwade/NARSAD/Aim_1/figures/gradient_boosted_trees_algorithm.png)

Unlike RFs and SVMs, gradient boosted trees have a lot of parameters to tune and therefore grid searche spaces are extensive. Manual tuning is difficult so I'll compare that performance to adaptive grid searches. 

[Gradient Boosted Tree Outline](https://xgboost.readthedocs.io/en/latest/parameter.html)

* **nrounds**: Controls the maximum number of iterations. For classification, it is similar to the number of trees to grow.
* **eta**: Step size shrinkage used in update. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. Range 0:1. 
* **max depth**: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. Range 0:Inf. 
* **colsample by tree**: The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.
* **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Range 0:Inf
* **child weight**: Minimum sum of instance weight needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. Range 0:Inf. 
* **subsample**: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. This will prevent overfitting. Subsampling will occur once in every boosting iteration. Range 0:1.


# Within-arm predictions: Manual grid search results
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
fid <- dir("/ifshome/bwade/NARSAD/Aim_1/results/refinement/", pattern = 'Rdata', full.names = TRUE)

perf2 <- lapply(fid, function(f){
  
  print(f)
  
  load(f)
    
  pvac <- sapply(fits, function(x) cor(x$mod$pred$pred, x$mod$pred$obs))
  
  rm(fits)
  
  ss <- strsplit(f, '_')[[1]]
  
  tmp <- data.frame(
    y=ss[grep('hdrs6|rrsr', ss)],
    baseline=gsub('baseline-','',ss[grep('baseline', ss)]),
    cutoff=gsub('cutoff-','',ss[grep('cutoff-', ss)]),
    ndthresh=gsub('ndthresh-','',ss[grep('ndthresh-', ss)]),
    nrounds=gsub('nrounds-','',ss[grep('nrounds-', ss)]),
    eta=gsub('eta-','',ss[grep('eta-', ss)]),
    maxdepth=gsub('depth-','',ss[grep('depth-', ss)]),
    gamma=gsub('gamma-','',ss[grep('gamma-', ss)]),
    colsamp=gsub('colsamp-','',ss[grep('colsamp-', ss)]),
    childweight=gsub('childwei-','',ss[grep('childwei-', ss)]),
    subsample=gsub('subsamp-|.Rdata','',ss[grep('subsamp-', ss)]),
    perf_e = pvac[[1]],
    perf_k = pvac[[2]],
    perf_s = pvac[[3]]
  )
  
  return(tmp)
  
    
})

perf <- do.call(rbind, perf2)
m_manual <- melt(perf, id.vars=c('y', 'baseline', 'cutoff', 'ndthresh', 'nrounds', 'eta', 'maxdepth', 'gamma', 'colsamp', 'childweight', 'subsample'))

ggplot(m_manual, aes(x=NULL, y=value, fill=variable)) + 
    geom_boxplot() + 
    facet_grid(y~baseline) + 
    geom_hline(yintercept = 0, col='red') + 
    ylim(-.4, .6) + 
    xlab('Treatment Group') + 
    ylab('Predicted vs. Actual Change Correlation') +
    ggtitle('PvAc by Treatment, Outcome, and Baseline Symptom', subtitle = 'Manual Grid Search') + 
    theme_bw()
```

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
grid <- expand.grid(classifier=c('xgbTree'),
                    cutoff=c(.1, .2, .3),
                    ndthresh=c(.5, .7), # global conn measures with n proportion == 0 are excluded
                    baseline=c(FALSE, TRUE),
                    y=c('rrsr', 'hdrs6'),
                    rmterms=TRUE,
                    nrounds = c(50, 100, 500, 1000),
                    eta=c(.025, .3),
                    max_depth=c(2, 4, 6, 12),
                    gamma = 0,
                    colsample_bytree=c(.5, 1),
                    min_child_weight = c(1,2,4),
                    subsample = c(.5, 1))

knitr::kable(table(outcome=grid$y, baeline_included=grid$baseline), caption='Number of Parameterizations per Model')
```

# Within-arm predictions: Adaptive grid search results
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
fid_adapt <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/adaptive_gridsearch/', pattern='Rdata', full.names = TRUE)

perf_adapt <- lapply(fid_adapt, function(f){
  
  load(f)
  
  cm <- sapply(fits, function(x){
    cor(x$mod$pred$pred, x$mod$pred$obs)
  })
  
  ss <- strsplit(f, '_')[[1]]
  
  tmp <- data.frame(
    y=ss[grep('rrsr|hdrs6', ss)],
    baseline=gsub('baseline-','', ss[grep('baseline', ss)]),
    cutoff=gsub('cutoff-','',ss[grep('cutoff-', ss)]),
    ndthresh=gsub('ndthresh-', '',ss[grep('ndthresh-', ss)]),
    #group=gsub('group-','',ss[grep('group-', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]]
  )
  return(tmp)
})

perf_adapt <- do.call(rbind, perf_adapt)

m_adapt <- melt(perf_adapt, id.vars=c('y', 'baseline', 'cutoff', 'ndthresh'))

ggplot(m_adapt, aes(x=NULL, y=value, fill=variable)) + 
    geom_boxplot() + 
    facet_grid(y~baseline) + 
    geom_hline(yintercept = 0, col='red') + 
    ylim(-.4, .6) + 
    xlab('Treatment Group') + 
    ylab('Predicted vs. Actual Change Correlation') +
    ggtitle('PvAc by Treatment, Outcome, and Baseline Symptom', subtitle = 'Adaptive Grid Search') + 
    theme_bw()
```


# Comparison of manual and adaptive model performances
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}

adapt <- ddply(m_adapt, .(baseline, variable, y), summarise, max=max(value, na.rm=TRUE))

manual <- ddply(m_manual, .(baseline, variable, y), summarise, max=max(value, na.rm=TRUE))

adapt$type = rep('adaptive', nrow(adapt))
manual$type = rep('manual', nrow(manual))

dfm <- rbind(adapt, manual)

ggplot(dfm, aes(y, max, fill=type)) + 
  geom_bar(stat='identity', position = "dodge") + 
  facet_grid(baseline~variable) + 
  ylab('Maximum PvAc') + 
  xlab('Symptom') + 
  ggtitle('Comparison of Highest Performing Models')
```
**Results**: Biggest gains for adaptive approach are for ECT arm. Adaptive method does poorly with sleep deprivation arm. 

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
## Plotting Functions ## 
# Function to plot PvAc
pvac_plot <- function(fit){
  
  pred <- fit$pred
  
  pvac <- round(cor(pred$obs, pred$pred), digits = 4)
  cp <- round(cor.test(pred$obs, pred$pred)$p.value, digits = 4)
  xpos <- max(pred$obs) - 2
  ypos <- max(pred$pred)
  
  p <- ggplot(pred, aes(obs, pred)) +
    geom_boxplot(aes(group=rowIndex)) + 
    geom_point(alpha=.5) + 
    geom_smooth(method=lm, col='red') + 
    geom_text(x=xpos, y=ypos, label=sprintf('r = %s; p = %s', pvac, cp)) + 
    ylab('Predicted Change') + xlab('Actual Change') + 
    ylim(-9, 2) + 
    theme_bw() + 
    ggtitle('Predicted Versus Actual Symptom Change')
  
  return(p)
  
}

# Function to plot the partial dependance plots for top n features
pdp_plot <- function(fit, n=6){
  
  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  
  plots <- lapply(c(features)[[1]], function(f){
    
    roi_probe <- as.character(f)
    
    plt <- partial(fit, pred.var = roi_probe, ice = TRUE, center = TRUE, 
                   plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", 
                   train = fit$trainingData, smooth=FALSE)
    
    return(plt)
    
  })
  
  plot_grid <- grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], nrow=2)
  
  return(plot_grid)
}


# Function to grab names of between-network features in important feature set and display those components
plot_components <- function(fit, n=6){
  
  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  features <- c(features[[1]])
  
  connfeatures <- grep('BnConn', features, value = T)
  
  if(length(connfeatures)==0){
    return(NULL)
  }else{
    
    for(k in 1:length(connfeatures)){
      
      components <- strsplit(connfeatures[k], '_')[[1]]
      c1 <- as.numeric(gsub('V', '', components[[1]])) - 1 # zero indexing
      c2 <- as.numeric(gsub('V', '', components[[2]])) - 1
      
      c1 <- paste0(str_pad(c1, 4, pad=0), '.png')
      c2 <- paste0(str_pad(c2, 4, pad=0), '.png')
      
      c1_path <- grep(c1, dir('/ifshome/bwade/NARSAD/Aim_1/figures/ICA_Components_200_NOV2019/Figures200d/', full.names = TRUE), value = T)
      c2_path <- grep(c2, dir('/ifshome/bwade/NARSAD/Aim_1/figures/ICA_Components_200_NOV2019/Figures200d/', full.names = TRUE), value = T)
      
      img1 <-  rasterGrob(as.raster(readPNG(c1_path)), interpolate = FALSE, height=.5)
      img2 <-  rasterGrob(as.raster(readPNG(c2_path)), interpolate = FALSE, height=.5)
      connarrow <-  rasterGrob(as.raster(readPNG('/ifshome/bwade/NARSAD/Aim_1/figures/conn_arrow.png')), interpolate = FALSE, height=.1)
      
      gridExtra::grid.arrange(img1, connarrow, img2, ncol = 3)#, widths=c(5, 1, 5))
      
      
      
    }
    
  }
  
}


# Function to display ICA components with important global connectivity properties
plot_global_conn <- function(fit, n=6){
  
  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  features <- c(features[[1]])
  
  g_connfeatures <- grep('Node_Degree', features, value = T)
  
  if(length(g_connfeatures)==0){
    return(NULL)
  }else{
    
    for(k in 1:length(g_connfeatures)){
      
      components <- strsplit(g_connfeatures[k], '_')[[1]]
      c1 <- as.numeric(gsub('V', '', components[[1]])) - 1 # zero indexing
      c1 <- paste0(str_pad(c1, 4, pad=0), '.png')
      
      c1_path <- grep(c1, dir('/ifshome/bwade/NARSAD/Aim_1/figures/ICA_Components_200_NOV2019/Figures200d/', full.names = TRUE), value = T)
      
      img1 <-  rasterGrob(as.raster(readPNG(c1_path)), interpolate = FALSE, height=.5)
      
      gridExtra::grid.arrange(img1)#, widths=c(5, 1, 5))
      
    }
    
  }
  
}

fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/', pattern = 'Rdata', full.names = TRUE)

# perf2 <- lapply(fid, function(f){
# 
#   load(f)
# 
#   pvac <- sapply(fits, function(p){
#     cor(p$mod$pred$pred, p$mod$pred$obs)
#   })
# 
#   rm(fits)
# 
#   ss <- strsplit(f, '_')[[1]]
# 
#   tmp <- data.frame(
#     y=ss[grep('hdrs6|rrsr', ss)],
#     baseline=gsub('baseline-','',ss[grep('baseline', ss)]),
#     cutoff=gsub('cutoff-','',ss[grep('cutoff-', ss)]),
#     ndthresh=gsub('ndthresh-','',ss[grep('ndthresh-', ss)]),
#     nrounds=gsub('nrounds-','',ss[grep('nrounds-', ss)]),
#     eta=gsub('eta-','',ss[grep('eta-', ss)]),
#     maxdepth=gsub('depth-','',ss[grep('depth-', ss)]),
#     gamma=gsub('gamma-','',ss[grep('gamma-', ss)]),
#     colsamp=gsub('colsamp-','',ss[grep('colsamp-', ss)]),
#     childweight=gsub('childwei-','',ss[grep('childwei-', ss)]),
#     subsample=gsub('subsamp-|.Rdata','',ss[grep('subsamp-', ss)]),
#     perf_e = pvac[[1]],
#     perf_k = pvac[[2]],
#     perf_s = pvac[[3]]
#   )
# 
#   return(tmp)
# 
# })
# perf <- do.call(rbind, perf2)
# perf_fid <- data.frame(perf, fid=fid)
load('/ifshome/bwade/NARSAD/Aim_1/data/perf_manual_fid.Rdata')

load_best_model <- function(y, arm, baseline, perf_fid){
  
  if(arm=='e'){
    idx <- 1
  }else if(arm=='k'){
    idx <- 2
  }else if(arm=='s'){
    idx <- 3
  }
  
  perf_ss <- perf_fid[perf_fid$y==y & perf_fid$baseline==baseline, ]
  load_idx <- which.max(perf_ss[[paste0('perf_', arm)]])
  load(as.character(perf_ss$fid[load_idx]))
  
  return(fits[[idx]])
  
}

plot_baseline_outcome_associations <- function(fit, y, arm, baseline, n=6){
  
  require(plyr)
  
  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  features <- c(features[[1]])
  
  dat_fid <- dir('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/', pattern = 'Rdata', full.names = TRUE)
  
  data_to_load <- grep(y, dat_fid, value = T)
  load(data_to_load)
  
  df <- plyr::rename(df, replace=c('3rd_Ventricle'='X3rd_Ventricle','4th_Ventricle'='X4th_Ventricle', '5th_Ventricle'='X5th_Ventricle'))
  
  df_ss <- df[df$group==arm, c('outcome', features)]
  
  par(mfrow=c(2, 3))
  
  for(c in 2:ncol(df_ss)){
    
    plot(df_ss[, c], df_ss[['outcome']], pch=19,
         ylab = paste(y, 'delta'), 
         xlab = colnames(df_ss)[c],
         main = sprintf('%s Delta vs. %s Baseline', y, colnames(df_ss)[c]))
    abline(lm(df_ss[['outcome']]~df_ss[,c]), col='red', lwd=2)
    
  }
  
}


compare_predictors <- function(y, nstart=1, nend=10, title){
  
  require(fmsb)
  
  load('/ifshome/bwade/NARSAD/Aim_1/data/perf_manual_fid.Rdata')
  
  perf_fid <- perf_fid[perf_fid$baseline==TRUE, ]
  
  m <- melt(perf_fid[, c('y', 'perf_e', 'perf_k', 'perf_s', 'fid')], id.vars=c('y', 'fid'))
  
  ds <- split(m, f=list(m$y, m$variable))
  
  # ordered list of performance by outcome and group
  dso <- lapply(ds, function(x){
    
    tmp <- x[order(x$value, decreasing = T), ]
    return(tmp[1:30, ])
    
  })
  
  if(y=='hdrs6'){
    fidlist <- list(dso[[1]]$fid, dso[[3]]$fid, dso[[5]]$fid) # HDRS Outcomes  
  }else if(y=='rrsr'){
    fidlist <- list(dso[[2]]$fid, dso[[4]]$fid, dso[[6]]$fid) # RRSR Outcomes 
  }
  
  ilist <- lapply(1:3, function(g){ # iterate over groups
    
    implist <- lapply(fid, function(f){ # iterate over group-specific fids
      
      load(as.character(f))
      
      imp <- xgb.importance(feature_names=fits[[g]]$mod$finalModel$feature_names, model=fits[[g]]$mod$finalModel)
      ret <- data.frame(feature=imp$Feature, gain=scale(imp$Gain, center=F))
      return(ret)
      
    })

    test <- Reduce(function(...) merge(..., all=T, by = 'feature'), implist)
    
    d <- data.frame(test$feature, imp=rowMeans(test[, 2:ncol(test)], na.rm=T))
    d <- d[order(d$imp, decreasing = T),]
    
    return(d)
    
  })
  
  features <- unique(c(as.character(ilist[[1]]$test.feature[nstart:nend]), as.character(ilist[[2]]$test.feature[nstart:nend]), as.character(ilist[[3]]$test.feature[nstart:nend])))
  
  data <- as.data.frame(matrix(rep(0, 3*length(features)), nrow=3))
  colnames(data) <- features
  rownames(data) <- c('E', 'K', 'S')
  
  for(r in 1:nrow(data)){
    #print(r)
    
    for(c in 1:ncol(data)){
      #print(c) 
      gain <- ilist[[r]][which(ilist[[r]]$test.feature %in% colnames(data)[c]), 'imp']
      
      if(length(gain)==1){
        data[r, c] <- gain  
      }else{
        data[r, c] <- 0
      }
      
    }
    
  }
  
  #data <- scale(data, center = F)
  
  maxval <- max(data) + sd(unlist(data))
  
  data <- rbind(rep(maxval,length(features)) , rep(0,length(features)) , data)
  
  # Color vector
  colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
  colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )
  
  # plot with default options:
  radarchart( as.data.frame(data)  , axistype=1 , 
              #custom polygon
              pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
              #custom the grid
              cglcol="grey", cglty=1, axislabcol="grey", 
              #caxislabels=seq(0,20,5), 
              cglwd=0.8,
              #custom labels
              vlcex=4,
              title=title
  )
  
  # Add a legend
  legend(x=1, y=1.2, legend = rownames(data[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=10)
  
  
}

```

# Outline of top-performing models

### ECT: RRS, Reflection; baseline symptoms included
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}

fit <- load_best_model(y = 'rrsr', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
fit <- fit$mod

p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

plot_baseline_outcome_associations(fit = fit, y = 'rrsr', arm = 'e', baseline = TRUE, n=6)

```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_global_conn(fit)
```

### ECT: HDRS-6; baseline symptoms included
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}

fit <- load_best_model(y = 'hdrs6', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
fit <- fit$mod

p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

plot_baseline_outcome_associations(fit = fit, y = 'hdrs6', arm = 'e', baseline = TRUE, n=6)

```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_global_conn(fit)
```

### Ketamine: RRS, Reflection; baseline symptoms included
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}

fit <- load_best_model(y = 'rrsr', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
fit <- fit$mod

p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

plot_baseline_outcome_associations(fit = fit, y = 'rrsr', arm = 'k', baseline = TRUE, n=6)

```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_global_conn(fit)
```

### Ketamine: HDRS-6; baseline symptoms included
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}

fit <- load_best_model(y = 'hdrs6', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
fit <- fit$mod

p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

plot_baseline_outcome_associations(fit = fit, y = 'hdrs6', arm = 'k', baseline = TRUE, n=6)

```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_global_conn(fit)
```

### Sleep Deprivation: RRS. RRSR didn't significantly change in TSD group so we can ignore this. 

### Sleep Deprivation: HDRS-6; baseline symptoms included
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}

fit <- load_best_model(y = 'hdrs6', arm = 's', baseline = TRUE, perf_fid = perf_fid)
fit <- fit$mod

p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

plot_baseline_outcome_associations(fit = fit, y = 'hdrs6', arm = 's', baseline = TRUE, n=6)

```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_global_conn(fit)
```

# Cross-treatment prediction: Manual grid search

**Approach**: Take models that performed decently within treatment (PvAc > 0.1) and use them to predict symptom change across arms. Do models performing better within-treatment perform better across treatment as well? 

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}

load('/ifshome/bwade/NARSAD/Aim_1/results/cross_correlation_results_with_parameters.Rdata')

df$basemodel <- paste0('basemodel_', df$basemodel)
df$crosstx <- paste0('predicting_', df$crosstx)

df_hdrs <- df[df$y=='hdrs6', ]
df_rrsr <- df[df$y=='rrsr', ]

ggplot(df_rrsr, aes(base, cross, fill=baseline)) + 
  geom_point(aes(colour=baseline)) + 
  facet_grid(basemodel~crosstx) + 
  geom_hline(yintercept = 0, col='black', lty=2) + 
  geom_vline(xintercept = 0, col='black', lty=2) + 
  theme_bw() + 
  xlab('Base Model Performance') + 
  ylab('Cross Treatment Performance') +
  ggtitle('Cross-treatment Symptom Change Predictions', subtitle = 'RRS: Rumination')

ggplot(df_hdrs, aes(base, cross, fill=baseline)) + 
  geom_point(aes(colour=baseline)) + 
  facet_grid(basemodel~crosstx) + 
  geom_hline(yintercept = 0, col='black', lty=2) + 
  geom_vline(xintercept = 0, col='black', lty=2) + 
  theme_bw() + 
  xlab('Base Model Performance') + 
  ylab('Cross Treatment Performance') +
  ggtitle('Cross-treatment Symptom Change Predictions', subtitle = 'HDRS-6')

```
**Results**: For both the RRSR and HDRS-6, higher performing ketamine models tend to do better across treatment arms. Sleep deprivation models generally perform poorly within arm but good models still generalize comparably. ECT models don't tend to generalize. Overall, generalizability is clearly highly dependent on inclusion of baseline symptoms. Some parameterizations, eta and child weight, were highly determinant (not shown).

## Overlap of important features

**Approach**: Evaluate top 30 models for each treatment-by-outcome combination and evaluate importance of each feature. Average feature importance score across 30 models and compare overlap of top 10 features. 
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
compare_predictors(y = 'rrsr', nstart = 1, nend = 10, title = 'Predictor Importance Overlap: RRSR Top 10')
```

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
compare_predictors(y = 'hdrs6', nstart = 1, nend = 10, title = 'Predictor Importance Overlap: HDRS6 Top 10')
```
**Results**: Results suggest that there is little overlap in the top features picked by these models. Gradient boosted trees involve a lot of resampling/subsampling. Given the small number of observations and roughly 16K features available to select from this isn't too surprising. Model generalizability is more important. 


```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
# Function for error analysis plots
error_analysis_plots <- function(fit, arm, y){
  
  # variance by subject
  pred <- fit$mod$pred
  psplit <- split(pred, f=pred$rowIndex)
  
  swe <- sapply(psplit, function(f){
    caret::RMSE(f$pred, f$obs)
  })
  
  low_error <- which(swe<=median(swe))
  high_error <- which(swe>median(swe))
  
  load(grep(y, dir('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/', full.names = TRUE), value = TRUE))
  
  df_tx <- df[df$group==arm, ]
  df_tx$error <- ifelse(swe <= median(swe), 'Low Error', 'High Error')
  
  require(reshape2)
  m <- melt(df_tx[, c('outcome', 'outcome_baseline', 'cont_age', 'cont_sex', 'dabst_duration', 'dabst_num_episodes', 'error')])
  
  p1 <- ggplot(m, aes(error, value, fill=error)) + 
    geom_boxplot() + 
    facet_wrap(~variable, scales = 'free') + 
    theme_bw() + 
    ggtitle("Demographic & Clinical Measures by Error Group") + 
    scale_fill_manual(values=c("#0073C2FF", "#EFC000FF"))
  
  ## Filter within feature type
  featurelist <- list(
    grep('lh_|rh_', names(df_tx), value = T),
    grep('Right_|Left_', names(df_tx), value = T),
    grep('_FA|_AD|_MD|_RD|_kurt', names(df_tx), value = T),
    grep('Degree', names(df_tx), value = T),
    grep('BnConn', names(df_tx), value = T)
  )
  
  pca_plots <- lapply(featurelist, function(f){
    
    pc_tmp <- prcomp(df_tx[, f], center = TRUE, scale = TRUE)
    
    ptmp <- fviz_pca_ind(pc_tmp, geom.ind = "point", pointshape = 21, 
                 pointsize = 2, 
                 fill.ind = df_tx$error,
                 col.ind = "black", 
                 palette = "jco", 
                 addEllipses = TRUE,
                 label = "var",
                 col.var = "black",
                 repel = TRUE,
                 legend.title = "Error Class") +
      ggtitle("PCA of Imagign Data by Error Rate") +
      theme(plot.title = element_text(hjust = 0.5))
    
    return(ptmp)
    
  })
  
  gridExtra::grid.arrange(p1, pca_plots[[1]]+ggtitle("PCA of Cortical Thickness by Error Group"),
                          pca_plots[[2]]+ggtitle("PCA of Subcortical Volume by Error Group"),
                          pca_plots[[3]]+ggtitle("PCA of Diffusion by Error Group"),
                          pca_plots[[4]]+ggtitle("PCA of Global Connectivity by Error Group"),
                          pca_plots[[5]]+ggtitle("PCA of Between-network Connectivity by Error Group"),
                          ncol=3)

}

# Scatter plots by confidence level
plots_by_confidence_level <- function(fit){

  # variance by subject
  pred <- fit$mod$pred
  psplit <- split(pred, f=pred$rowIndex)
  
  #boxplot(pred$pred ~ pred$rowIndex)
  
  swv <- sapply(psplit, function(f){
    sd(f$pred)
  })
  
  split_point <- quantile(swv, .5) # lower quantile = more confidence
  
  low_var <- which(swv<=split_point)
  high_var <- which(swv>split_point)
  
  pred_low_var <- pred[pred$rowIndex %in% low_var, ]
  pred_low_var$confidence <- rep('High', nrow(pred_low_var))
  
  pred_high_var <- pred[pred$rowIndex %in% high_var, ]
  pred_high_var$confidence <- rep('Low', nrow(pred_high_var))
  
  mdf <- rbind(pred_high_var, pred_low_var)
  mdf$confidence <- as.factor(mdf$confidence)
  mdf$confidence <- factor(mdf$confidence, levels=c('Low', 'High'))
  
  plt <- ggplot(mdf, aes(obs, pred, colour=confidence)) + 
    geom_point() + 
    geom_smooth(method = 'lm') + 
    theme_bw() + 
    ggtitle("PvAc by Confidence Level", subtitle = "Median SD Split") + 
    ylab("Predicted Change") + 
    xlab("Observed Change")
  
  return(plt)

}


```

# Error Analysis of Best Models
**Approach**: Median split subjects by their RMSE across 10-repeated 10-fold cross validation folds. Compare demographic and imaging characteristics of subjects with high and low errors. 

### ECT RRSR
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'rrsr', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 'e', y = 'rrsr')
```
**Results**: The model struggled with participants with higher baseline symptoms who subsequently reduced symptoms more, younger participants. Imaging measures were similarly distributed across high and low error participants. Structural imaging measures more similar than functional measures due to a few outliers. 

### ECT HDRS
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'hdrs6', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 'e', y = 'hdrs6')
```
**Results**: The model did worse with participants that had fewer baseline symptoms (unlike the RRSR) and who were older with a longer symptom duration. Global connectivity measures distributed more orthogonally between error categories. 

### Ketamine RRSR
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'rrsr', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 'k', y = 'rrsr')
```
**Results**: Model did worse with participants that improved more but baseline symptoms didn't make a difference. Age, symptom duration, and number of prior episodes somewhat different between classes. Barring a few outliers in functional data, imaging measures were similar.


### Ketamine HDRS
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'hdrs6', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 'k', y = 'hdrs6')
```
**Results**: Similarly, model did worse with participants that did better. Age remains different. Global connectivity measures more orthogonal in PCA space. 


### TSD RRSR
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'rrsr', arm = 's', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 's', y = 'rrsr')
```
**Results**: Demographic & clinical measures strongly differe by error class. Functional imaging space skewed by several outiers. 

### TSD HDRS
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'hdrs6', arm = 's', baseline = TRUE, perf_fid = perf_fid)
error_analysis_plots(fit = fit, arm = 's', y = 'hdrs6')
```
**Results**: Same as RRSR.

# Performance by Confidence Level

Maybe if we only offer predictions about patients that we have a lot of confidence about, the models will do better overall? My naive approach was to say the model is more confident about a patient when their repeated predictions across cross validation are more similar, i.e., lower standard deviation. 

```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=20, fig.height=20}
fit <- load_best_model(y = 'rrsr', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
p1 <- plots_by_confidence_level(fit)
p1 <- p1 + ggtitle("ECT RRSR")

fit <- load_best_model(y = 'hdrs6', arm = 'e', baseline = TRUE, perf_fid = perf_fid)
p2 <- plots_by_confidence_level(fit)
p2 <- p2 + ggtitle("ECT HDRS6")

fit <- load_best_model(y = 'rrsr', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
p3 <- plots_by_confidence_level(fit)
p3 <- p3 + ggtitle("Ketamine RRSR")

fit <- load_best_model(y = 'hdrs6', arm = 'k', baseline = TRUE, perf_fid = perf_fid)
p4 <- plots_by_confidence_level(fit)
p4 <- p4 + ggtitle("Ketamine HDRS6")

fit <- load_best_model(y = 'rrsr', arm = 's', baseline = TRUE, perf_fid = perf_fid)
p5 <- plots_by_confidence_level(fit)
p5 <- p5 + ggtitle("TSD RRSR")

fit <- load_best_model(y = 'hdrs6', arm = 's', baseline = TRUE, perf_fid = perf_fid)
p6 <- plots_by_confidence_level(fit)
p6 <- p6 + ggtitle("TSD HDRS6")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol=2)
```
**Result**: Seems like PvAc is higher when we only predict high-variance patients. 

# Next Steps

* Evaluate directionality of top predictors within arm in other treatment arms. 
* Personalized Advantage Index to directly evaluate treatment stratification

![](/ifshome/bwade/NARSAD/Aim_1/figures/pai.png)










