{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "## Resources: \n",
    "# Custom transformer: https://medium.com/analytics-vidhya/scikit-learn-pipelines-with-custom-transformer-a-step-by-step-guide-9b9b886fd2cc\n",
    "# https://nilearn.github.io/auto_examples/03_connectivity/plot_group_level_connectivity.html\n",
    "# See https://rmldj.github.io/hcp-utils/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "import os\n",
    "import hcp_utils as hcp\n",
    "import nibabel as nib\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from chord import Chord\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "import statsmodels.stats.multicomp as mc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "#print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "atlas_name='yeo17'\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "if atlas_name=='yeo17':\n",
    "    labs=['VIS_1', 'VIS_2', 'MOT_1', 'MOT_2', 'DAN_1', 'DAN_2', 'SAL_VATTN_1', 'SAL_VATTN_2', 'LIM_1', 'LIM_2', 'FPN_1', 'Control_1', 'Control_2', 'TEMP_PAR', 'DMN_1', 'FPN_2', 'DMN_2', 'Hb_L', 'Hb_R']\n",
    "elif atlas_name=='yeo7':\n",
    "    for i, v in enumerate(hcp.yeo7.labels.values()):\n",
    "        #print(i, v.replace(\" \", \"_\"))\n",
    "        hcp.yeo7.labels[i]=v.replace(\" \", \"_\")\n",
    "    labs=[getattr(hcp, atlas_name).labels[x] for x in getattr(hcp, atlas_name).labels if x > 0]\n",
    "    labs.extend(['Hb_L', 'Hb_R'])\n",
    "else:\n",
    "    labs=[getattr(hcp, atlas_name).labels[x] for x in getattr(hcp, atlas_name).labels if x > 0]\n",
    "    labs.extend(['Hb_L', 'Hb_R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographic/clinical data\n",
    "\n",
    "dem_path='/ifshome/bwade/NARSAD/Aim_2/data/'\n",
    "dem=pd.read_csv(dem_path+'U01_Demographics_IDS.csv')\n",
    "\n",
    "# Get iterable list of subject IDs with time points\n",
    "sid_list=dem['Subject'].tolist()\n",
    "arm_list=dem['arm'].tolist()\n",
    "sid_list=[x for x in sid_list]\n",
    "arm_list=[x for x in arm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 e0010\n",
      "Normalized data contains NAN Values... passing\n",
      "1 e0011\n",
      "2 e0012\n",
      "3 e0014\n",
      "4 e0015\n",
      "5 e0016\n",
      "6 e0017\n",
      "7 e0018\n",
      "8 e0019\n",
      "9 e0021\n",
      "10 e0022\n",
      "11 e0023\n",
      "12 e0025\n",
      "13 e0026\n",
      "14 e0028\n",
      "15 e0031\n",
      "16 e0032\n",
      "17 e0033\n",
      "18 e0034\n",
      "19 e0035\n",
      "20 e0037\n",
      "21 e0038\n",
      "22 e0041\n",
      "23 e0043\n",
      "24 e0046\n",
      "25 e0047\n",
      "26 e0048\n",
      "27 e0050\n",
      "28 e0053\n",
      "29 e0056\n",
      "30 e0058\n",
      "31 e0059\n",
      "32 k0004\n",
      "33 k0008\n",
      "34 k0009\n",
      "35 k0010\n",
      "36 k0011\n",
      "37 k0014\n",
      "38 k0017\n",
      "39 k0019\n",
      "40 k0026\n",
      "41 k0027\n",
      "42 k0031\n",
      "43 k0035\n",
      "44 k0036\n",
      "45 k0047\n",
      "46 k0049\n",
      "47 k0050\n",
      "48 k0051\n",
      "49 k0053\n",
      "50 k0055\n",
      "51 k0056\n",
      "52 k0060\n",
      "53 k0066\n",
      "54 k0067\n",
      "55 k0071\n",
      "56 k0074\n",
      "57 k0075\n",
      "58 k0076\n",
      "59 k0078\n",
      "60 k0085\n",
      "61 k0089\n",
      "62 k0090\n",
      "63 k0094\n",
      "64 k0103\n",
      "65 k0105\n",
      "66 k0114\n",
      "67 k0115\n",
      "68 k0117\n",
      "69 k0124\n",
      "70 k0125\n",
      "71 k0131\n",
      "72 k0133\n",
      "73 k0139\n",
      "74 k0144\n",
      "75 k0147\n",
      "76 k0149\n",
      "77 k0151\n",
      "78 k0155\n",
      "79 k0157\n",
      "80 k0166\n",
      "81 k0167\n",
      "82 k0172\n",
      "83 k0191\n",
      "84 k0192\n",
      "85 k0195\n",
      "86 k0196\n",
      "87 k0199\n",
      "88 k0203\n",
      "89 k0205\n",
      "90 k0206\n",
      "91 k0207\n",
      "92 s0030\n",
      "93 s0032\n",
      "94 s0034\n",
      "95 s0035\n",
      "96 s0043\n",
      "97 s0044\n",
      "98 s0045\n",
      "99 s0046\n",
      "100 s0049\n",
      "101 s0050\n",
      "102 s0054\n",
      "103 s0055\n",
      "104 s0063\n",
      "105 s0066\n",
      "106 s0069\n",
      "107 s0071\n",
      "108 s0073\n",
      "109 s0075\n",
      "110 s0077\n",
      "111 s0081\n",
      "112 s0090\n",
      "113 s0097\n",
      "114 s0099\n",
      "115 s0102\n",
      "116 s0104\n",
      "117 s0105\n",
      "118 s0108\n",
      "119 s0110\n",
      "120 s0112\n",
      "121 s0118\n",
      "122 s0119\n",
      "123 s0120\n",
      "124 s0123\n",
      "125 s0124\n",
      "126 s0125\n",
      "127 s0132\n",
      "128 s0133\n",
      "129 s0136\n",
      "130 s0137\n",
      "131 s0141\n",
      "132 s0142\n",
      "133 s0144\n",
      "134 s0145\n",
      "135 s0152\n",
      "136 s0155\n",
      "137 s0160\n",
      "138 s0161\n",
      "139 s0166\n",
      "140 s0177\n",
      "141 s0179\n"
     ]
    }
   ],
   "source": [
    "# Function to load and concatenate time series from whole brain and habenula\n",
    "def img_loader(img_file, hbl_file, hbr_file):\n",
    "    img=nib.load(img_file)\n",
    "    img=img.get_fdata() # get data\n",
    "    hbr=nib.load(hbr_file)\n",
    "    hbr=hbr.get_fdata()\n",
    "    hbl=nib.load(hbl_file)\n",
    "    hbl=hbl.get_fdata()\n",
    "    img_cat=np.concatenate((img, hbl, hbr), axis=1)  \n",
    "    return img_cat\n",
    "\n",
    "# RSFC data generator\n",
    "def write_baseline_rsfc(sid_list, root_dir, hb_root_dir, atlas):\n",
    "    \n",
    "    for index, subj in enumerate(sid_list):\n",
    "        \n",
    "        print(index, subj)\n",
    "                \n",
    "        im_file_ap_baseline=root_dir + subj + '01' + '/MNINonLinear/Results/rest_acq-AP_run-01/rest_acq-AP_run-01_Atlas_MSMAll_Test_hp2000_clean.dtseries.nii'\n",
    "        im_file_pa_baseline=root_dir + subj + '01' + '/MNINonLinear/Results/rest_acq-PA_run-02/rest_acq-PA_run-02_Atlas_MSMAll_Test_hp2000_clean.dtseries.nii'\n",
    "        \n",
    "        hb_r_file_ap_baseline=hb_root_dir + subj + '01/' + 'HbR_{}_MeanTS_drest-AP.sdseries.nii'.format(subj+'01')\n",
    "        hb_l_file_ap_baseline=hb_root_dir + subj + '01/' + 'HbL_{}_MeanTS_drest-AP.sdseries.nii'.format(subj+'01')\n",
    "        hb_r_file_pa_baseline=hb_root_dir + subj + '01/' + 'HbR_{}_MeanTS_drest-PA.sdseries.nii'.format(subj+'01')\n",
    "        hb_l_file_pa_baseline=hb_root_dir + subj + '01/' + 'HbL_{}_MeanTS_drest-PA.sdseries.nii'.format(subj+'01')            \n",
    "\n",
    "        if path.exists(im_file_ap_baseline) and path.exists(im_file_pa_baseline) and path.exists(hb_l_file_ap_baseline):\n",
    "            \n",
    "            # load ap acquisitions: baseline\n",
    "            img_ap_baseline_cat=img_loader(img_file=im_file_ap_baseline, \n",
    "                                           hbl_file=hb_l_file_ap_baseline, \n",
    "                                           hbr_file=hb_r_file_ap_baseline)\n",
    "\n",
    "            # load pa acquisitions: baseline\n",
    "            img_pa_baseline_cat=img_loader(img_file=im_file_pa_baseline, \n",
    "                               hbl_file=hb_l_file_pa_baseline, \n",
    "                               hbr_file=hb_r_file_pa_baseline)                    \n",
    "            \n",
    "            # average baseline runs and normalize\n",
    "            img_baseline=np.mean((img_ap_baseline_cat, img_pa_baseline_cat),axis=0)\n",
    "            img_baseline_norm=hcp.normalize(img_baseline) \n",
    "            \n",
    "            if np.sum(np.isnan(img_baseline_norm)) >0: \n",
    "                print('Normalized data contains NAN Values... passing')\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "\n",
    "\n",
    "                ## Atlas Parcellations\n",
    "                # get regional time series\n",
    "                hb_timeseries_baseline=img_baseline_norm[:, -2:]            \n",
    "\n",
    "                img_timeseries_baseline=hcp.parcellate(img_baseline_norm[:,:-2], getattr(hcp, atlas))\n",
    "\n",
    "                # compute correlation matrices\n",
    "                correlation_measure=ConnectivityMeasure(kind='partial correlation')\n",
    "                correlation_matrix_baseline=correlation_measure.fit_transform([np.concatenate((img_timeseries_baseline, hb_timeseries_baseline), axis=1)])[0]                        \n",
    "\n",
    "                # save matrix\n",
    "                outfile='/ifshome/bwade/NARSAD/Aim_1/data/atlas_rsfc/' + subj + '_{}_atlas'.format(atlas)\n",
    "                np.savetxt(outfile, correlation_matrix_baseline, delimiter=',')\n",
    "\n",
    "        else: pass\n",
    "\n",
    "write_baseline_rsfc(sid_list=sid_list, root_dir='/nafs/narr/canderson/new_pipeline_test_runs/out/', hb_root_dir='/nafs/narr/HCP_OUTPUT/Habenula/outputs/RSConn_HbSeed/ROI_Timeseries/HbROIs/', atlas=atlas_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
