---
title: "NARSAD Aim 1"
author: "Benjamin Wade"
date: "4/23/2020"
output: html_document
---

```{r setup, include=FALSE}
require(ggplot2)
require(caret)
require(reshape2)
require(randomForest)
require(xgboost)
require(pdp)
require(png)
require(grid)
require(gridExtra)
require(stringr)
require(plyr)
require(knitr)
require(pander)
```

# Outline

* Project Aims
* Demographics
* Grid Search
    + Coarse search & hyperparameters
      - Model Selection
      - Feature Selection Parameter
    + Refinement
  
* ECT models within treatment arm
    + Reflection
    + Brooding
    + TCQR
    + HDRS-6
    
* Ketamine models within treatment arm
    + Reflection
    + Brooding
    + TCQR
    + HDRS-6
    
* Sleep deprivation models within treatment arm
    + Reflection
    + Brooding
    + TCQR
    + HDRS-6
    
* Model Performances Across Treatment Arms


# 1) Project Aims
**Aim 1**: Predict degree of improvement in ruminative and depressive symptoms: RRS & HDRS-6.

**Aim 2**: Determine which treatment-specific models predict across treatment arms.


# 2) Sample Outline
```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.align='center', fig.width=10}
load('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/rumq_rrsr_total_st_end_of_treatment.Rdata')

knitr::kable(ddply(df, .(group), summarise, n=length(cont_sex), mean_age=mean(cont_age), sd_age=sd(cont_age), n_males=sum(cont_sex))) #rssr=mean(outcome_baseline), rssr_percent_change=mean(outcome/outcome_baseline)))

l <- lapply(dir('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/', pattern = '.Rdata', full.names = TRUE), function(f){
  load(f)
  return(data.frame(percent_change=(df$outcome/df$outcome_baseline), group=df$group ,scale=rep(strsplit(f,'/|_')[[1]][12], nrow(df))))
})

tmpdf <- do.call(rbind, l)
tmpdf$scale <- as.character(tmpdf$scale)
tmpdf$scale[tmpdf$scale=='end'] <- 'hdrs6'

ggplot(tmpdf, aes(scale, percent_change, fill=scale)) + 
  geom_boxplot() + 
  theme_bw() + 
  ggtitle('Percent Symptom Change to End of Treatment')

ggplot(tmpdf, aes(scale, percent_change, fill=scale)) + 
  geom_boxplot() + 
  facet_wrap(~group) + 
  theme_bw() + 
  ggtitle('Percent Symptom Change to End of Treatment by Treatment Arm')

# testing significant differences between scales and treatments
tdf <- tmpdf[complete.cases(tmpdf), ]
tdf <- tdf[!is.infinite(tdf$percent_change), ]
a <- aov(percent_change ~ group*scale, data = tdf)

panderOptions('table.split.table', Inf)

pander(summary(a), caption = '2-way ANOVA Table: Treatment-by-Scale Changes')

pander( ph$`group:scale`[ph$`group:scale`[,'p adj'] < 0.05,], caption = 'Significant Treatment-by-Scale Change Differences')
```

# 3) Model Selection

**Goal**:Train and test several types of classifiers (Random Forests, Radial SVM, Gradient Boosted Trees) to narrow down which are best 
```{r, echo=FALSE, cache=TRUE, warning=FALSE}
## Functions inherited from one_shot_pass_evaluate.R
# Get coarse tuning grid
grid <- expand.grid(classifier=c('rf', 'xgbTree', 'svmRadial'),
                    cutoff=c(.1, .3, .5, .7, .9),
                    baseline=c(TRUE, FALSE),
                    y=c('rumq_rrsb_total_st', 'rumq_rrsr_total_st', 'rumq_tcqr_total_st', 'hdrs6'),
                    diff_drop=TRUE,
                    rmterms=TRUE)

# Read in and organize model performances

perf <- lapply(1:nrow(grid), function(r){
  
  #print(r)
  
  flist <- dir('/ifshome/bwade/NARSAD/Aim_1/results/gridsearch/', full.names = TRUE)
  
  f <- grep(grid[r, 'y'], grep(paste0('baseline-', grid[r, 'baseline']), grep(paste0(grid[r, 'cutoff'],'_'), grep(grid[r, 'classifier'], flist, value = T), value = T), value = T), value = T)
  
  if(length(f)==0){
    return(NA)
  }else{
    
    
    load(f)
    
    cm <- list()
    
    for(a in 1:3){
      
      pred <- fits[[a]]$mod$pred
      
      cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
      
    }
    
    return(cm)
  }
  
})

grid$performance_ect <- sapply(perf, function(x) tryCatch(x[[1]], error=function(e) NA))
grid$performance_ski <- sapply(perf, function(x) tryCatch(x[[2]], error=function(e) NA))
grid$performance_tsd <- sapply(perf, function(x) tryCatch(x[[3]], error=function(e) NA))

m <- melt(grid, id.vars=c('classifier', 'cutoff', 'baseline','y','diff_drop', 'rmterms'))
```

### 3a) Coarse search & hyperparameters

#### Performance by classifier and use of baseline symptoms
```{r, echo=FALSE, warning=FALSE, fig.align='center', fig.width=15}
ggplot(m, aes(classifier, value, fill=classifier)) + 
  geom_boxplot() + facet_grid(baseline~y) + theme_bw() + 
  ylab('Predicted vs. Actual Change Correlation') + xlab('Classifier')
```
**Result**: Gradient boosted trees consistently outperform both random forests and radial SVMs

### Performance by classifier and correlation threshold hyperparameter
```{r, echo=FALSE, warning=FALSE, fig.align='center', fig.width=10}
ggplot(m, aes(as.factor(cutoff), value, fill=as.factor(cutoff))) + 
  geom_boxplot() + facet_wrap(~y) + theme_bw() + 
  ylab('Predicted vs. Actual Change Correlation') + 
  xlab('Correlation Cutoff')
```
**Result**: A correlation threshold between 0.1 and 0.3 looks best. Now a more refined grid search using boosted trees and a strict cutoff can be done. 

# 3b) Refined grid search
```{r, echo=FALSE, warning=FALSE, cache=TRUE}

fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/', full.names = TRUE, pattern='Rdata')

perf <- lapply(fid, function(f){
  
    load(f)
    
    cm <- list()
    
    for(a in 1:3){
      
      pred <- fits[[a]]$mod$pred
      
      cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
      
    }
    
    ss <- strsplit(f, '/|_')[[1]]
    
    if(length(grep('hdrs', f))==0){
      
      tdf <- data.frame(
        cutoff=gsub('cutoff-','',ss[11]),
        baseline=gsub('baseline-','',ss[12]),
        y=ss[14],
        nrounds=gsub('nrounds-','',ss[18]),
        eta=gsub('eta-','',ss[19]),
        maxdepth=gsub('depth-', '', ss[21]),
        gamma=gsub('gamma-', '',ss[22]),
        colsamp=gsub('colsamp-', '', ss[23]),
        childweight=gsub('childwei-', '', ss[24]),
        subsamp=gsub('.Rdata|subsamp-','',ss[25]),
        perf_e=cm[[1]],
        perf_k=cm[[2]],
        perf_s=cm[[3]])
      
      return(tdf)
      
      }else{
        
        tdf <- data.frame(
          cutoff=gsub('cutoff-','',ss[11]),
          baseline=gsub('baseline-','',ss[12]),
          y=ss[13],
          nrounds=gsub('nrounds-','',ss[15]),
          eta=gsub('eta-','',ss[16]),
          maxdepth=gsub('depth-', '', ss[18]),
          gamma=gsub('gamma-', '',ss[19]),
          colsamp=gsub('colsamp-', '', ss[20]),
          childweight=gsub('childwei-', '', ss[21]),
          subsamp=gsub('.Rdata|subsamp-','',ss[22]),
          perf_e=cm[[1]],
          perf_k=cm[[2]],
          perf_s=cm[[3]])
        
        return(tdf)
        
      }
  
})

pdf <- do.call(rbind, perf)
pdf$nrounds <- droplevels(factor(pdf$nrounds, levels = c(5, 10, 25, 50, 100, 500, 1000, 2000, 3000, 4000, 5000)))
pdf$maxdepth <- droplevels(factor(pdf$maxdepth, levels = c(2, 4, 6, 8, 12, 16, 20, 30)))
pdf$eta <- droplevels(factor(pdf$eta, levels= seq(.025, .85, .05)))
pdf$childweight <- droplevels(factor(pdf$childweight, levels=c(1,2,3)))
pdf$colsamp <- droplevels(factor(pdf$colsamp, levels=c(0.2, 0.4, 0.6, 0.8, 1)))
pdf$gamma <- droplevels(factor(pdf$gamma, levels=c(seq(0, 1, .05))))

m <- melt(pdf, id.vars = c('cutoff', 'baseline', 'y', 'nrounds', 'eta', 'maxdepth', 'gamma', 'colsamp', 'childweight', 'subsamp'))
```

Gradient boosted trees have several important parameters to tune:

[Gradient Boosted Tree Outline](https://xgboost.readthedocs.io/en/latest/parameter.html)

* **nrounds**: Controls the maximum number of iterations. For classification, it is similar to the number of trees to grow.
* **eta**: Step size shrinkage used in update. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. Range 0:1. 
* **max depth**: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. Range 0:Inf. 
* **colsample by tree**: The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.
* **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. Range 0:Inf
* **child weight**: Minimum sum of instance weight needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be. Range 0:Inf. 
* **subsample**: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. This will prevent overfitting. Subsampling will occur once in every boosting iteration. Range 0:1.


Because there are so many model-specific parameters and hyperparameters to tune in a brute force grid search, I'll tune some of the larger knobs first...

### Tuning the nrounds first holding other parameters at constant values
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/nrounds/', full.names=TRUE)
fid <- fid[grepl('rrsr', fid)]

perf <- lapply(fid, function(f){

  load(f)
    
  cm <- list()
  
  for(a in 1:3){
    
    pred <- fits[[a]]$mod$pred
    
    cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
    
  }
  
  ss <- strsplit(f, '/|_')[[1]]
    
  tdf <- data.frame(
    nrounds=gsub('nrounds-','',ss[grep('nrounds-', ss)]),
    baseline=gsub('baseline-','',ss[grep('baseline-', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]])
  
})
  
pdf <- do.call(rbind, perf)
pdf$nrounds <- droplevels(factor(pdf$nrounds, levels = c(25, 50, 100, 500, 1000, 2000, 3000, 4000, 5000)))

m <- melt(pdf, id.vars = c('nrounds', 'baseline'))

ggplot(m, aes(nrounds, value, colour=variable, group=1)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(baseline~variable) + 
  theme( axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```
**Result**: Differences are more pronounced for models excluding baseline severity. I'll restrict the nrounds range to: {100, 500, 1000}


### Tuning the max depth parameter holding other parameters constant (nrounds set to 100)
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/maxdepth/', full.names=TRUE)
fid <- fid[grepl('hdrs', fid)]

perf <- lapply(fid, function(f){

  load(f)
    
  cm <- list()
  
  for(a in 1:3){
    
    pred <- fits[[a]]$mod$pred
    
    cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
    
  }
  
  ss <- strsplit(f, '/|_')[[1]]
    
  tdf <- data.frame(
    maxdepth=gsub('depth-','',ss[grep('depth-', ss)]),
    baseline=gsub('baseline-','',ss[grep('baseline-', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]])
  
})
  
pdf <- do.call(rbind, perf)
pdf$maxdepth <- droplevels(factor(pdf$maxdepth, levels = c(2, 4, 6, 8, 10, 40, 50)))

m <- melt(pdf, id.vars = c('maxdepth', 'baseline'))

ggplot(m, aes(maxdepth, value, colour=variable, group=1)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(baseline~variable) 
  

```
**Result**: Given the above, I restricted this range to 4:6


### Tuning the child weight parameter
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/childweight/', full.names=TRUE)
fid <- fid[grepl('rrsr', fid)]

perf <- lapply(fid, function(f){

  load(f)
    
  cm <- list()
  
  for(a in 1:3){
    
    pred <- fits[[a]]$mod$pred
    
    cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
    
  }
  
  ss <- strsplit(f, '/|_')[[1]]
    
  tdf <- data.frame(
    childweight=gsub('childwei-','',ss[grep('childwei-', ss)]),
    baseline=gsub('baseline-','',ss[grep('baseline-', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]])
  
})
  
pdf <- do.call(rbind, perf)
pdf$childweight <- droplevels(factor(pdf$childweight, levels=c(1,2,3)))

m <- melt(pdf, id.vars = c('childweight', 'baseline'))

ggplot(m, aes(childweight, value, colour=variable, group=1)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(baseline~variable)

```
**Result**: No clear winner here; I restricted the range to 2. 


### Tuning eta
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/eta/', full.names=TRUE)
fid <- fid[grepl('rrsr', fid)]

perf <- lapply(fid, function(f){

  load(f)
    
  cm <- list()
  
  for(a in 1:3){
    
    pred <- fits[[a]]$mod$pred
    
    cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
    
  }
  
  ss <- strsplit(f, '/|_')[[1]]
    
  tdf <- data.frame(
    eta=gsub('eta-','',ss[grep('eta-', ss)]),
    baseline=gsub('baseline-','', ss[grep('baseline-', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]])
  
})
  
pdf <- do.call(rbind, perf)
pdf$eta <- droplevels(factor(pdf$eta, levels=seq(.025, .85, .05)))

m <- melt(pdf, id.vars = c('eta', 'baseline'))

ggplot(m, aes(eta, value, colour=variable, group=1)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(baseline~variable) + ylab('Predicted vs. Actual Change Correlation') +
  theme( axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```
**Result**: Biggest effect is for models including baseline symptoms but general trend is strongly downard for all so I set eta to 0.025. 


### Tuning gamma
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/gamma/', full.names=TRUE)
fid <- fid[grepl('rrsr', fid)]

perf <- lapply(fid, function(f){

  load(f)
    
  cm <- list()
  
  for(a in 1:3){
    
    pred <- fits[[a]]$mod$pred
    
    cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
    
  }
  
  ss <- strsplit(f, '/|_')[[1]]
    
  tdf <- data.frame(
    gamma=gsub('gamma-','', ss[grep('gamma', ss)][2]),
    baseline=gsub('baseline-','',ss[grep('baseline', ss)]),
    perf_e=cm[[1]],
    perf_k=cm[[2]],
    perf_s=cm[[3]])
  
})
  
pdf <- do.call(rbind, perf)
pdf$gamma <- droplevels(factor(pdf$gamma, levels=c(seq(0, 1, .05))))

m <- melt(pdf, id.vars = c('gamma', 'baseline'))

ggplot(m, aes(gamma, value, colour=variable, group=1)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(baseline~variable) + ylab('Predicted vs. Actual Change Correlation') +
  theme( axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ddply(m, .(gamma), summarise, mean=mean(value))

```
**Result**: No clear effect. Set to 0.55. 

### Grid search over restricted range of parameters

```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/refinement/', full.names = TRUE, pattern='Rdata')

perf <- lapply(fid, function(f){
  
    load(f)
    
    cm <- list()
    
    for(a in 1:3){
      
      pred <- fits[[a]]$mod$pred
      
      cm[[a]] <- tryCatch(cor(pred$pred, pred$obs), error = function(e) NA)
      
    }
    
    ss <- strsplit(f, '/|_')[[1]]
    
    if(length(grep('hdrs', f))==0){
      
      tdf <- data.frame(
        cutoff=gsub('cutoff-','',ss[grep('cutoff-', ss)]),
        baseline=gsub('baseline-','',ss[grep('baseline-', ss)]),
        y=ss[grep('rrsr|rrsb|tcqr', ss)],
        nrounds=gsub('nrounds-','',ss[grep('nrounds-', ss)]),
        eta=gsub('eta-','',ss[grep('eta-', ss)]),
        maxdepth=gsub('depth-', '', ss[grep('depth-', ss)]),
        gamma=gsub('gamma-', '',ss[grep('gamma-', ss)]),
        colsamp=gsub('colsamp-', '', ss[grep('colsamp-', ss)]),
        childweight=gsub('childwei-', '', ss[grep('childwei-', ss)]),
        subsamp=gsub('.Rdata|subsamp-','',ss[grep('subsamp-', ss)]),
        diffdrop=gsub('diffdrop-', '', ss[grep('diffdrop-', ss)]),
        perf_e=cm[[1]],
        perf_k=cm[[2]],
        perf_s=cm[[3]])
      
      return(tdf)
      
      }else{
        
        tdf <- data.frame(
          cutoff=gsub('cutoff-','',ss[grep('cutoff-', ss)]),
          baseline=gsub('baseline-','',ss[grep('baseline-', ss)]),
          y=ss[grep('hdrs', ss)],
          nrounds=gsub('nrounds-','',ss[grep('nrounds-', ss)]),
          eta=gsub('eta-','',ss[grep('eta-', ss)]),
          maxdepth=gsub('depth-', '', ss[grep('depth-', ss)]),
          gamma=gsub('gamma-', '',ss[grep('gamma-', ss)]),
          colsamp=gsub('colsamp-', '', ss[grep('colsamp-', ss)]),
          childweight=gsub('childwei-', '', ss[grep('childwei-', ss)]),
          subsamp=gsub('.Rdata|subsamp-','',ss[grep('subsamp-', ss)]),
          diffdrop=gsub('diffdrop-', '', ss[grep('diffdrop-', ss)]),
          perf_e=cm[[1]],
          perf_k=cm[[2]],
          perf_s=cm[[3]])
        
        return(tdf)
        
      }
  
})

pdf <- do.call(rbind, perf)
pdf$nrounds <- droplevels(factor(pdf$nrounds, levels = c(100, 500, 1000)))
pdf$maxdepth <- droplevels(factor(pdf$maxdepth, levels = c(2, 4, 6)))
pdf$eta <- droplevels(factor(pdf$eta, levels= (.025)))
pdf$childweight <- droplevels(factor(pdf$childweight, levels=c(1,2)))
pdf$colsamp <- droplevels(factor(pdf$colsamp, levels=c(0.4)))
pdf$gamma <- droplevels(factor(pdf$gamma, levels=c(0.3, 0.55)))
```

### Performance for RRS and HDRS6 across restricted range of parameters without baseline symptoms
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
m <- melt(pdf, id.vars = c('cutoff', 'baseline', 'y', 'nrounds', 'eta', 'maxdepth', 'gamma', 'colsamp', 'childweight', 'subsamp'))
ggplot(m, aes(y, value, fill=y)) + 
  geom_boxplot() + 
  facet_grid(~baseline) + 
  ggtitle('Performance by Outcome and Baseline Symptom Inclusion')

m <- m[m$subsamp==1 & m$childweight==1 & m$maxdepth==4 & m$baseline==FALSE, ]
ggplot(m, aes(nrounds, value, colour=y, group=y)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(cutoff~variable) + 
  ggtitle('Performance by outcome, arm, and hyperparameter')
```
**Results**: Performance varies a lot across scales, treatments, and parameterization. This suggests that no one parameterization will be optimal for all outcomes and treatments. I'll outline the models that perform the best within this restricted grid search. 

```{r, echo=FALSE}
m <- melt(pdf, id.vars = c('cutoff', 'baseline', 'y', 'nrounds', 'eta', 'maxdepth', 'gamma', 'colsamp', 'childweight', 'subsamp'))
mf <- m[m$baseline==FALSE, ]
ms <- split(mf, list(mf$y, mf$variable))
performance <- do.call(rbind, lapply(ms, function(x) x[which.max(x$value),]))
performance <- plyr::rename(performance, replace=c('value'='PvAc Score'))
knitr::kable(performance, format="pandoc",
             caption="Highest Performing Models by Arm and Symptom Set")
```

# 4) ECT Models, No Baseline Symptoms
```{r, echo=FALSE, warning=FALSE, cache=TRUE}
# Function to plot PvAc
pvac_plot <- function(fit){
  pred <- fit$pred

  pvac <- round(cor(pred$obs, pred$pred), digits = 4)
  cp <- round(cor.test(pred$obs, pred$pred)$p.value, digits = 4)
  xpos <- max(pred$obs) - 2
  ypos <- max(pred$pred)
  
  p <- ggplot(pred, aes(obs, pred)) +
    geom_boxplot(aes(group=rowIndex)) + 
    geom_point(alpha=.5) + 
    geom_smooth(method=lm, col='red') + 
    geom_text(x=xpos, y=ypos, label=sprintf('r = %s; p = %s', pvac, cp)) + 
    ylab('Predicted Change') + xlab('Actual Change') + 
    theme_bw() + 
    ggtitle('Predicted Versus Actual Symptom Change')
  
  return(p)

}

# Function to plot the partial dependance plots for top n features
pdp_plot <- function(fit, n=6){

  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  
  plots <- lapply(c(features)[[1]], function(f){
    
    roi_probe <- as.character(f)
    
    plt <- partial(fit, pred.var = roi_probe, ice = TRUE, center = TRUE, 
            plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", 
            train = fit$trainingData, smooth=FALSE)
    
    return(plt)
    
  })
  
  plot_grid <- grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], plots[[5]], plots[[6]], nrow=2)

  return(plot_grid)
}


# Function to grab names of between-network features in important feature set and display those components
plot_components <- function(fit, n=6){

  xgbimp <- xgb.importance(feature_names=fit$finalModel$feature_names, model=fit$finalModel)
  
  features <- xgbimp[1:n, 'Feature']
  features <- c(features[[1]])
  
  connfeatures <- grep('BnConn', features, value = T)
  
  if(length(connfeatures)==0){
    return(NULL)
  }else{
    
    for(k in 1:length(connfeatures)){
      
      components <- strsplit(connfeatures[k], '_')[[1]]
      c1 <- as.numeric(gsub('V', '', components[[1]])) - 1 # zero indexing
      c2 <- as.numeric(gsub('V', '', components[[2]])) - 1
      
      c1 <- paste0(str_pad(c1, 4, pad=0), '.png')
      c2 <- paste0(str_pad(c2, 4, pad=0), '.png')
      
      c1_path <- grep(c1, dir('/ifshome/bwade/NARSAD/Aim_1/figures/ICA_Components_200_NOV2019/Figures200d/', full.names = TRUE), value = T)
      c2_path <- grep(c2, dir('/ifshome/bwade/NARSAD/Aim_1/figures/ICA_Components_200_NOV2019/Figures200d/', full.names = TRUE), value = T)
      
      img1 <-  rasterGrob(as.raster(readPNG(c1_path)), interpolate = FALSE, height=.5)
      img2 <-  rasterGrob(as.raster(readPNG(c2_path)), interpolate = FALSE, height=.5)
      connarrow <-  rasterGrob(as.raster(readPNG('/ifshome/bwade/NARSAD/Aim_1/figures/conn_arrow.png')), interpolate = FALSE, height=.1)
      
      gridExtra::grid.arrange(img1, connarrow, img2, ncol = 3)#, widths=c(5, 1, 5))
      
      
      
    }
  
  }

}


```

## 4a) RRS: Reflection
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_e_cutoff-0.1_baseline-FALSE_rrsr_nrounds-500_eta-0.025_max_depth-2_gamma-0.3_colsamp-0.4_childwei-2_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

## 4b) RRS: Brooding
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_e_cutoff-0.1_baseline-FALSE_rrsb_nrounds-100_eta-0.025_max_depth-2_gamma-0.3_colsamp-0.4_childwei-1_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```


## 4c) Rumination: TCQR
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_e_cutoff-0.1_baseline-FALSE_tcqr_nrounds-500_eta-0.025_max_depth-6_gamma-0.3_colsamp-0.4_childwei-2_subsamp-0.5.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

## 4d) HDRS 6-item scale
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_e_cutoff-0.3_baseline-FALSE_hdrs6_nrounds-500_eta-0.025_max_depth-6_gamma-0.3_colsamp-0.4_childwei-1_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)
```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```


# 5) Ketamine Models, No Baseline Symptoms

## 5a) RRS: Reflection
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_k_cutoff-0.3_baseline-FALSE_rrsr_nrounds-100_eta-0.025_max_depth-2_gamma-0.3_colsamp-0.4_childwei-2_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

## 5b) RRS: Brooding
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_k_cutoff-0.1_baseline-FALSE_rrsb_nrounds-100_eta-0.025_max_depth-4_gamma-0.3_colsamp-0.4_childwei-2_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```



## 5c) RRS: TCRQ
### PvAc for this model was < 0 so not worth outlining results

## 5d) HDRS 6-item Scale
### PvAc for this model very low, < 0.1, so not worth outlining results. 


# 6) Sleep Deprivation Models, No Baseline Symptoms

## 6a) RRS: Reflection
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_s_cutoff-0.3_baseline-FALSE_rrsr_nrounds-100_eta-0.025_max_depth-4_gamma-0.3_colsamp-0.4_childwei-2_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

## 6b) RRS: Brooding
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_s_cutoff-0.3_baseline-FALSE_rrsb_nrounds-500_eta-0.025_max_depth-2_gamma-0.3_colsamp-0.4_childwei-1_subsamp-1.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```

## 6c) RRS: TCQR
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10}
rm(fit)
load('/ifshome/bwade/NARSAD/Aim_1/results/final_models/perf_s_cutoff-0.1_baseline-FALSE_tcqr_nrounds-500_eta-0.025_max_depth-2_gamma-0.3_colsamp-0.4_childwei-2_subsamp-0.5.Rdata')
p1 <-pvac_plot(fit)
p2 <-plot(varImp(fit), main = 'Variable Importance')

grid.arrange(p1, p2, ncol=2)

pdp_plot(fit)

```

### Frequently selected ICA components
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.align='center'}
plot_components(fit)
```


## 6d) HDRS 6-item Scale

### PvAc < 0; results not worth outlining

# 7) Performance of Models Across Treatment Arms
```{r, echo=FALSE, warning=FALSE, cache=TRUE}
grid <- expand.grid(classifier=c('rf', 'xgbTree', 'svmRadial'),
                    cutoff=c(.1, .3, .5, .7, .9),
                    baseline=c(TRUE, FALSE),
                    y=c('rumq_rrsb_total_st', 'rumq_rrsr_total_st', 'rumq_tcqr_total_st', 'hdrs6'),
                    diff_drop=TRUE,
                    rmterms=TRUE)

load('/ifshome/bwade/NARSAD/Aim_1/data/compiled_datasets/hdrs6_end_of_treatment.Rdata')

perf <- lapply(1:nrow(grid), function(r){
  
  flist <- dir('/ifshome/bwade/NARSAD/Aim_1/results/gridsearch/', full.names = TRUE)
  
  f <- grep(grid[r, 'y'], grep(paste0('baseline-', grid[r, 'baseline']), grep(paste0(grid[r, 'cutoff'],'_'), grep(grid[r, 'classifier'], flist, value = T), value = T), value = T), value = T)
  
  if(length(f)==0){
    return(NA)
  }else{
    
    
    load(f)
    
    cm <- matrix(rep(NA, 9), 3,3)
    
    groups <- c('e', 'k', 's')
    for(i in 1:3){
      
      dfred <- df[df$group==groups[i], ]
      
      for(j in 1:3){
        
        pred <- tryCatch(predict(fits[[j]]$mod, newdata=dfred), error=function(e) NA)
        cm[i, j] <- tryCatch(cor(pred, dfred$outcome), error=function(e) NA)
        
      }
      
    }
    
    }
    
    return(cm)
  
})


# Reference map
cmr <- matrix(rep(NA, 9), 3,3)

groups <- c('e', 'k', 's')
for(i in 1:3){

  new=groups[i]
  #dfred <- df[df$group==groups[i], ]

  for(j in 1:3){

    obs=groups[j]

    #pred <- predict(fits[[j]]$mod, newdata=dfred)
    cmr[i, j] <- paste0('trained_on_',obs,':','tested_on_',new)

  }

}

l <- list()
u <- 1
for(i in 1:3){
  for(j in 1:3){
    l[[u]] <- sapply(perf, function(p) tryCatch(p[i,j], error=function(e) NA))
    u <- u+1
  }
}


u <- 1
for(i in 1:3){
  for(j in 1:3){
    names(l)[u] <- cmr[i, j]
    u <- u+1
  }
}


perf_df <- data.frame(do.call(cbind, l))


grid <- data.frame(grid, perf_df)


m <- melt(grid, id.vars=c('classifier', 'cutoff', 'baseline','y','diff_drop', 'rmterms'))
m_ss <- m[!m$variable %in% c('trained_on_e.tested_on_e', 'trained_on_k.tested_on_k', 'trained_on_s.tested_on_s'),]
m_ss$cutoff <- droplevels(as.factor(m_ss$cutoff))
```

#### Coarse grid-search results by classifier
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=10, fig.height=7}
ggplot(m_ss, aes(classifier, value, fill=classifier)) + 
  geom_boxplot() + 
  ylab('Predicted vs. Actual Change Correlation')
m_ss <- m_ss[m_ss$classifier=='xgbTree',]
```
**Result**: Gradient boosted trees seem to generalize slightly better, on average. 


#### Cross-treatment performance of all models in refined grid search
```{r, echo=FALSE, warning=FALSE, cache=TRUE, fig.align='center', fig.width=15, fig.height=15}
fid <- dir('/ifshome/bwade/NARSAD/Aim_1/results/cross_treatment_grid_search/', full.names = TRUE)

df <- lapply(fid, function(f){
  
  load(f)
  
  if(grepl('predicting-e', f)){
    cm <- cor(e_pred$obs, e_pred$pred)
    rm(e_pred)
  }else if(grepl('predicting-k', f)){
    cm <- cor(k_pred$obs, k_pred$pred)
    rm(k_pred)
  }else if(grepl('predicting-s', f)){
    cm <- cor(s_pred$obs, s_pred$pred)
    rm(s_pred)
  }
  
  ss <- strsplit(f, '/|_')[[1]]
  
  tmp <- data.frame(
    basemodel=ss[grep('basemodel-', ss)],
    predicted=ss[grep('predicting-', ss)],
    cutoff=gsub('cutoff-','', ss[grep('cutoff-', ss)]),
    baseline=gsub('baseline-','', ss[grep('baseline-', ss)]),
    y=grep('hdrs|rrsr|rrsb|tcqr', ss, value=TRUE),
    nrounds=gsub('nrounds-', '', ss[grep('nrounds-', ss)]),
    eta=gsub('eta-','', ss[grep('eta-', ss)]),
    max_depth=gsub('depth-','',ss[grep('depth-', ss)]),
    gamma=gsub('gamma-','',ss[grep('gamma-', ss)]),
    colsamp=gsub('colsamp-','',ss[grep('colsamp-', ss)]),
    chidweight=gsub('childwei-','',ss[grep('childwei-', ss)]),
    subsample=gsub('subsamp-|.Rdata','', ss[grep('subsamp-', ss)]),
    perf=cm
  )
  
})


df <- do.call(rbind, df)

df$predicted <- factor(df$predicted, levels = c('predicting-e', 'predicting-k', 'predicting-s'))
df$nrounds <- factor(df$nrounds, levels = c(50, 100, 500))

# highlight best models within arm
model_list <- read.table('/ifshome/bwade/NARSAD/Aim_1/results/final_models/best_fits_all.txt')
df$best_within_arm_model <- rep(0, nrow(df))

# ect models
df[df$basemodel=='basemodel-e' & df$cutoff==0.3 & df$baseline==FALSE & df$y=='hdrs6' & df$nrounds==100 & df$eta==0.025 & df$max_depth==6 & df$gamma==0.3 & df$chidweight==1 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-e' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='rrsb' & df$nrounds==100 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==1 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-e' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='rrsr' & df$nrounds==500 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-e' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='tcqr' & df$nrounds==500 & df$eta==0.025 & df$max_depth==6 & df$gamma==0.3 & df$chidweight==2 & df$subsample==0.5, 'best_within_arm_model'] <- 1

# ketamine models
df[df$basemodel=='basemodel-k' & df$cutoff==0.3 & df$baseline==FALSE & df$y=='hdrs6' & df$nrounds==500 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==2 & df$subsample==0.5, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-k' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='rrsb' & df$nrounds==100 & df$eta==0.025 & df$max_depth==4 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-k' & df$cutoff==0.3 & df$baseline==FALSE & df$y=='rrsr' & df$nrounds==100 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-k' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='tcqr' & df$nrounds==100 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1

# sleep deprivation
df[df$basemodel=='basemodel-s' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='hdrs6' & df$nrounds==100 & df$eta==0.025 & df$max_depth==6 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-s' & df$cutoff==0.3 & df$baseline==FALSE & df$y=='rrsb' & df$nrounds==500 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==1 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-s' & df$cutoff==0.3 & df$baseline==FALSE & df$y=='rrsr' & df$nrounds==100 & df$eta==0.025 & df$max_depth==4 & df$gamma==0.3 & df$chidweight==2 & df$subsample==1, 'best_within_arm_model'] <- 1
df[df$basemodel=='basemodel-s' & df$cutoff==0.1 & df$baseline==FALSE & df$y=='tcqr' & df$nrounds==500 & df$eta==0.025 & df$max_depth==2 & df$gamma==0.3 & df$chidweight==2 & df$subsample==0.5, 'best_within_arm_model'] <- 1


df$best_within_arm_model <- as.factor(df$best_within_arm_model)

# test whether distributions are significant
df_split <- split(df, f=list(df$basemodel, df$predicted, df$y, df$baseline))

results <- lapply(df_split, function(x){
  res <- tryCatch(t.test(x[['perf']], alternative = 'greater', mu=0)$p.value, error=function(e) NA)
  tmp <- data.frame(x[1, ], sig=res)
  return(tmp)
}) 

res <- do.call(rbind, results)
res <- res[complete.cases(res), ]
res$sig_level <- ifelse(res$sig<0.05, 1, NA)
res$pos <- rep(0.8, nrow(res))


ggplot(df, aes(y, perf, fill=baseline)) + 
  geom_boxplot() + 
  facet_grid(basemodel~predicted) +
  geom_hline(yintercept=0, color='red') + 
  theme_bw() + 
  geom_point(data=df[df$best_within_arm_model==1,], aes(y, perf), color='green', shape=18, size=4) + 
  ggtitle('Cross Treatment Prediction Performance', subtitle = 'Results from refined grid search') + 
  ylab('Predicted Versus Actual Symptom Change Correlation') + 
  xlab('Symptom Dimension') + 
  labs(caption="Teal=Baseline Symptoms Included; \n Red=No Baseline Symptoms; \n Green Points=Performance of Best Within-arm Model") 
#+ geom_point(data=res, aes(y, pos), color='red', shape=8, size=3)

```
**Results**: Inclusion of baseline symptom severity greatly improves model generalizability, of course. However, the distribution of several model performances across treatment arms is consistently above zero despite excluding baseline severity. Also, the models optimized for performance within treatment arms (green diamonds) are not always the best at generalizing. 








